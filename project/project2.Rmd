---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "Tiffany Tran tkt474"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

# Modeling

####Introduction

- **0. (5 pts)** Introduce your dataset and each of your variables (or just your main variables if you have lots) in a paragraph. What are they measuring? How many observations?
```{R}
diabetes <- read.csv("diabetes.csv")
diabetes <- diabetes %>% select(Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, Age, Outcome)


head(diabetes)
```
The dataset `diabetes` was acquired from https://www.kaggle.com/datasets. The dataset was obtained from the National Institute of Diabetes and Digestive and Kidney Diseases, which was used to predict whether a patient has diabetes based on diagnostic measurements. The patients are all females from Pima Indian heritage who are at least 21 years old. Variables in the dataset includes number of times pregnant, glucose concentration, diastolic blood pressure (mmHg), tricep skin thickness (mm), insulin levels (muIU/mL), BMI (kg/m^2), age, and the categorical and binary variable of the outcome (1-diabetes or 0-no diabetes). There are 768 observations of 8 variables in the dataset. 

####MANOVA testing
```{R}
library(lmtest)
library(sandwich)
library(plotROC)
library(tidyverse)
library(MASS)
library(glmnet)
library(ggplot2)
manova <- manova(cbind(Glucose,BloodPressure,SkinThickness,Insulin,BMI,Age)~Pregnancies,data=diabetes)
summary(manova)

summary.aov(manova)

CatPreg <- cut(diabetes$Pregnancies, breaks = c(0,5,15), labels=c("low","high"), right=FALSE)

pairwise.t.test(diabetes$Glucose,CatPreg,p.adj="none")
pairwise.t.test(diabetes$BloodPressure,CatPreg,p.adj="none")
pairwise.t.test(diabetes$SkinThickness,CatPreg,p.adj="none")
pairwise.t.test(diabetes$Insulin,CatPreg,p.adj="none")
pairwise.t.test(diabetes$Age,CatPreg,p.adj="none")

#Type-1 error
1-(0.95)^12

#Bonferroni 1 MAN, 6 ANOVA, 5 pairwise = 12
0.05/12
pairwise.t.test(diabetes$Glucose,CatPreg,p.adj="bonf")
pairwise.t.test(diabetes$BloodPressure,CatPreg,p.adj="bonf")
pairwise.t.test(diabetes$SkinThickness,CatPreg,p.adj="bonf")
pairwise.t.test(diabetes$Insulin,CatPreg,p.adj="bonf")
pairwise.t.test(diabetes$Age,CatPreg,p.adj="bonf")
```

A MANOVA test was conducted to determine the effect of number of pregnancies on the person's glucose concentration, blood pressure, skin thickness, insulin, BMI, and age. After running the test, it can be concluded that the variables are statistically significant from each other since the p value is less than 0.05. Since the results were significant, a univariate ANOVA was conducted. The results from the ANOVA showed significance from pregnancies for all the variables, except for BMI. 

Post-hoc t-tests were run for all the variables to determine which category of number of  pregnancies (low=0-5, high=6-15) differed in each variable. Given the p-values, it is concluded that low and high number of pregnancies are significantly different with only the BMI variable, but are not significantly different from the other five variables. 

A total of 12 tests were performed (1 MANOVA, 6 ANOVA, 5 pairwise). The probability of a Type 1 error occuring was 0.4596399 or 45.96%. The Bonferroni adjusted significance level for 0.05 Type-1 error was 0.004166667. After adjusting for the significance level using the Bonferroni correction, the mean difference between low and high number of pregnancies remained the same for each of the variables as before the correction.

In conclusion, the assumptions for the MANOVA test were most likely not met because it has more assumptions that must be met such as having no extreme outliers, homogeneity, multivariate normality, etc.  

- **2. (10 pts)** Perform some kind of randomization test on your data (that makes sense). The statistic can be anything you want (mean difference, correlation, F-statistic/ANOVA, chi-squared), etc. State null and alternative hypotheses, perform the test, and interpret the results (7). Create a plot visualizing the null distribution and the test statistic (3).

Null: The mean age is the same (no difference) for patients with low and high number of pregnancies. 
Alternative: The mean age is different for patients with low and high number of pregnancies.
```{R}
low<-diabetes%>%filter(CatPreg=="low")
high<-diabetes%>%filter(CatPreg=="high")
mean(high$Age)-mean(low$Age)

rand_dist<-vector()
for(i in 1:5000) {
new<-data.frame(Age=sample(diabetes$Age),condition=CatPreg)
rand_dist[i]<-mean(new[new$condition=="high",]$Age)-
 mean(new[new$condition=="low",]$Age)}

t.test(data=diabetes, Age~CatPreg)

library(ggplot2)
hist(rand_dist, breaks = c(20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100), data=rand_dist);abline(v=-13.48608,col="red")

```
The mean difference was calculated for the randomization test as 13.48608 years. The p-value was significant, therefore we can reject the null hypothesis. This indicates that the mean age is different for patients with low and high number of pregnancies.

- **3. (40 pts)** Build a linear regression model predicting one of your response variables from at least 2 other variables, including their interaction. Mean-center any numeric variables involved in the interaction.
```{R}
diabetes_c <- diabetes
diabetes$Glucose <- diabetes_c$Glucose - mean(diabetes_c$Glucose, na.rm = T)
fit1 <-lm(BMI ~ Outcome * Glucose, data = diabetes)
summary(fit1)
library(ggplot2)
library(interactions)

ggplot(diabetes, aes(x=Glucose, y=BMI, group=Outcome))+geom_point(aes(color=Outcome))+
geom_smooth(method="lm",se=F,fullrange=T,aes(color=Outcome)) + ggtitle("Interaction between Glucose and Diabetics on BMI") + xlab("Glucose Concentration (mmol/L)")

resid <-fit1$residuals
fitvalue <- fit1$fitted.values

ggplot(diabetes, aes(x=Glucose, y=BMI)) + ggtitle("Interaction between Glucose and BMI")+ xlab("Glucose Concentration (mmol/L)")+ geom_point() + geom_smooth(method = "lm", se=F)

#Homoskedasticity
ggplot()+geom_point(aes(fitvalue,resid))+geom_hline(yintercept=0, color='red') 
bptest(fit1)

ggplot()+geom_histogram(aes(resid), bins=20)

ggplot()+geom_qq(aes(sample=resid))+geom_qq_line()
shapiro.test(resid)
coeftest(fit1, vcov = vcovHC(fit1))
summary(fit1)
```
When there is no interaction between outcome of diabetes and glucose concentration, the intercept estimate of 30.72720 is the average BMI. The coefficient estimate of 4.18187 is the amount that the average BMI increases when the patient has diabetes. The coefficient estimate of 0.03876 is the amount that the average BMI increases with the increase in one mmol/L of glucose concentration. The coefficient estimate of -0.02729 is the amount that the average BMI decreases if the patient had diabetes and glucose concentration decreases.
After plotting the data, it is clear that there is a relatively linear relationship between glucose concentration and BMI. The histogram data seemed quite off with a random break on the distribution, therefore there is no say that there is much of a normal distribution. According to the Breusch-Pagan Test, the p-value was not significant (0.7776), therefore we fail to reject the null hypothesis. On the other hand, the Shapiro-Wilk test shows a significant p-value (2.478e-16), meaning that the null hypothesis of normality was rejected. Although there were contrasting outcomes for the Breusch-Pagan Test and Shapiro-Wilk test, the linearity assumptions were relatively met, but the assumptions for homoskedasticity and the assumptions for normality were not met.
The regression results was recomputed with robust standard errors. The coefficient estimates did not change, however the standard errors and t-values slightly changed. The proportion of variation from the R-squared value is 0.09716. The adjusted R-squared is slightly lower at 0.09362. The slight difference in R-squared values shows that the variation in BMI is explained by the model. 

- **4. (5 pts)** Rerun same regression model (with the interaction), but this time compute bootstrapped standard errors (either by resampling observations or residuals). Discuss any changes you observe in SEs and p-values using these SEs compared to the original SEs and the robust SEs)
```{R}
samp_dist<-replicate(5000, {
boot_dat<-diabetes_c[sample(nrow(diabetes_c),replace=TRUE),]
fit2<-lm(BMI ~ Outcome * Glucose, data=boot_dat)
coef(fit2)
})
samp_dist%>%t%>%as.data.frame%>%summarize_all(sd)

fit3<-lm(BMI ~ Outcome * Glucose, data=diabetes_c)
resids <-fit3$residuals
fitted <- fit3$fitted.values
resid_resamp<-replicate(5000,{
new_resids<-sample(resids,replace=TRUE)
diabetes_c$new_y<-fitted+new_resids
fit3<-lm(new_y ~ Outcome * Glucose, data=diabetes_c)
coef(fit3)
})

resid_resamp%>%t%>%as.data.frame%>%summarize_all(sd)

```
After rerunning the same regression model with bootstrapped standard errors by resampling residuals, it was concluded that the new standard errors from bootstrapped model were slightly lower and higher than the standard errors from the original and robust standard errors. On the other hand, the new standard errors from the residual resampling had lower standard errors for all variables. 

- **5. (30 pts)** Fit a logistic regression model predicting a binary variable (if you don't have one, make/get one) from at least two explanatory variables (interaction not necessary). 
```{R}
library(lmtest)
diabetes_c <-diabetes_c %>%mutate(y=as.numeric(Outcome=="1"))

fit4<-glm(y ~ Age + Glucose,data=diabetes_c,family=binomial(link="logit"))
coeftest(fit4)
exp(0.0247784)
exp(0.0356440)

diabetes_c$prob <- predict(fit4, type = "response")
table(predict=as.numeric(diabetes_c$prob > 0.5), truth=diabetes_c$y) %>% addmargins

(436+130)/768 #accuracy
436/574 #sensitivity TPR
130/194 #specificity FPR
130/268 #precision

diabetes$logit<-predict(fit4) 
diabetes$Outcome<-as.factor(diabetes$Outcome)
ggplot(diabetes, aes(logit, fill=Outcome)) + geom_density(alpha=.3) + geom_vline(xintercept=0,lty=2)

ROCplot <- ggplot(diabetes_c) +geom_roc(aes(d=y, m=prob), n.cuts = 0) + geom_segment(aes(x=0, xend=1, y=0, yend=1))
ROCplot
calc_auc(ROCplot)
```
After running the logistic regression model, when controlling for glucose concentration, there is a significant difference of age on the outcome of having diabetes. Furthermore, when controlling for age, there is a significant difference of glucose concentration on the outcome of having diabetes. When there is no interaction between age and glucose concentration, the intercept estimate of -1.6032792 is the chance of having diabetes. Every one year increase in age multiplies the patients odds of having diabetes by 1.025088, and every one unit increase in glucose concentration multiplies the patients odds of having diabetes by 1.036287. 
For the confusion matrix, the accuracy of the model is 0.7369792, which indicates the number of patients that are predicted to have no diabetes that actually doesn't have diabetes. The sensitivity of the model is 0.7595819. The specificity of the model is 0.6701031. The precision of the model is 0.4850746. 
After computing the ROC curve, the AUC for the model was calculated to be 0.8007127. Since this is a relatively good AUC, it could be concluded that age and glucose concentrations are good indicators of the outcome of having diabetes.  


- **6. (25 pts)** Perform a logistic regression predicting the same binary response variable from *ALL* of the rest of your variables (the more, the better!) 
```{R}
fit5 <- glm(y ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI+ Age, data = diabetes_c, family = binomial(link = "logit"))
coeftest(fit5)

diabetes_c$prob <- predict(fit5, type ="response")
table(predict=as.numeric(diabetes_c$prob>.5),truth=diabetes_c$y)%>% addmargins()
(439+156)/768 #accuracy
156/268 #sensitivity
439/500 #specificity
156/217 #precision

class_diag<-function(probs,truth){
tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
acc=sum(diag(tab))/sum(tab)
sens=tab[2,2]/colSums(tab)[2]
spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]
if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
ord<-order(probs, decreasing=TRUE)
probs <- probs[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
data.frame(acc,sens,spec,ppv,auc)
}


#10-Fold CV
set.seed(1234)
k=10
data1<-diabetes_c[sample(nrow(diabetes_c)),]
folds<-cut(seq(1:nrow(diabetes_c)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
train<-data1[folds!=i,]
test<-data1[folds==i,]
truth<-test$y
fit<-glm(y ~ Age + Glucose,data=train,family="binomial")
probs<-predict(fit,newdata = test,type="response")
diags<-rbind(diags,class_diag(probs,truth))
}
apply(diags,2,mean)


library(glmnet)
fit6 <- diabetes%>%dplyr::select(Pregnancies,Glucose,BloodPressure,Insulin,BMI,Age,Outcome, SkinThickness)
y<-as.matrix(fit6$Outcome)
x<-model.matrix(Outcome~(.),data=fit6)[,-1]
cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso)

diabetes_new <- data.frame(diabetes_c$Pregnancies, diabetes_c$Glucose, diabetes_c$y)
data_new <- diabetes_new[sample(nrow(diabetes_new)), ]
folds <- cut(seq(1:nrow(diabetes_new)), breaks = k, labels = F)
diags <- NULL
for (i in 1:k) {
train <- data_new[folds != i, ]
test <- data_new[folds == i, ]
truth <- test$diabetes_c.y
fit <- lm(diabetes_c.y ~ ., data = train, family = "binomial")
probs <- predict(fit, newdata = test, type = "response")
diags <- rbind(diags, class_diag(probs, truth))
}

apply(diags,2,mean)
```
After computing a logisitic regression for all the variables in the dataset, the accuracy was 0.5562355 for patients who are actually diabetic. The sensitivity was 0.8553846 which indicates number of predicted non-diabetics. The specificity was 0.1926134 which includes both diabetic and non-diabetic patients. The precision was 0.5628955 which indicates patients who are actually non-diabetic. 
The same model was used for a 10-fold CV. The accuracy was 0.5534955, which was slightly lower than the in-sample metrics. The sensitivity was 0.8752451. The specificity was 0.1650197. The precision was 0.5602152. The AUC was 0.5479474. All of the values were lower than those from the in-sample metrics. The lower AUC indicates that the model is bad and isn't a great predictor of outcome of having diabetes. Furthermore, it shows signs of overfitting. 
From the LASSO regression, it is apparent that number of pregnancies and BMI are the most important predictors for the model and the rest are noise. The accuracy was 0.7526316, which was slightly higher than the logistic regressions. The sensitivity was 0.4932777. The specificity was 0.8905372. The precision was 0.7073179. The AUC was 0.7993912 The slightly higher AUC indicates that the model is a good fit and that number of pregnancies and glucose concentration is a good predictor of outcome of diabetes.


```{R eval=F}
data(package = .packages(all.available = TRUE))
```


...





